    #!/usr/bin/env python3
    """
    Complete GWO-K-Means vs Standard K-Means Clustering Analysis Application
    with comprehensive PCA analysis and advanced visualizations.
    """

    import sys
    import os
    import time
    import threading
    import tkinter as tk
    from tkinter import ttk, filedialog, messagebox, scrolledtext
    import numpy as np
    import pandas as pd
    import matplotlib
    matplotlib.use('TkAgg')
    import matplotlib.pyplot as plt
    plt.style.use('seaborn-v0_8')
    from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
    from matplotlib.figure import Figure
    from matplotlib.patches import Rectangle
    import seaborn as sns

    # Scientific computing imports
    from sklearn.impute import SimpleImputer
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.decomposition import PCA
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, pairwise_distances, silhouette_samples

    class GWOKMeansApp:
    def __init__(self, root):
        self.root = root
        self.root.title("PCA-Enhanced GWO-K-Means vs Standard K-Means Clustering Analysis")
        self.root.geometry("1600x1000")
        
        # Data variables
        self.df = None
        self.X_original = None
        self.X_preprocessed = None
        self.X_pca = None
        self.X_pca2 = None
        self.pca_model = None
        self.pca2_model = None
        self.explained_variance_ratio = None
        
        # Results for both methods
        self.gwo_labels = None
        self.gwo_centroids = None
        self.gwo_results = {}
        
        self.kmeans_labels = None
        self.kmeans_centroids = None
        self.kmeans_results = {}
        
        # Best method determination
        self.best_method = None
        self.best_score = None
        
        # Setup GUI
        self.setup_gui()
        
    def setup_gui(self):
        # Create notebook for tabs
        self.notebook = ttk.Notebook(self.root)
        self.notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # Tab 1: Data Upload and Configuration
        self.setup_data_tab()
        
        # Tab 2: PCA Analysis
        self.setup_pca_tab()
        
        # Tab 3: Results and Comparison
        self.setup_results_tab()
        
        # Tab 4: Detailed Evaluation
        self.setup_evaluation_tab()
        
        # Tab 5: Best Position Determination
        self.setup_best_position_tab()
        
        # Tab 6: Logs
        self.setup_logs_tab()
        
    def setup_data_tab(self):
        self.data_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.data_frame, text="Data & Configuration")
        
        # File upload section
        upload_frame = ttk.LabelFrame(self.data_frame, text="Data Upload", padding=10)
        upload_frame.pack(fill=tk.X, padx=10, pady=5)
        
        ttk.Button(upload_frame, text="Upload Data File (Excel/CSV)", 
                    command=self.upload_file).pack(side=tk.LEFT, padx=5)
        
        self.file_label = ttk.Label(upload_frame, text="No file selected")
        self.file_label.pack(side=tk.LEFT, padx=10)
        
        # Data preview
        preview_frame = ttk.LabelFrame(self.data_frame, text="Data Preview", padding=10)
        preview_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        
        # Treeview for data preview
        self.tree = ttk.Treeview(preview_frame)
        self.tree.pack(fill=tk.BOTH, expand=True)
        
        tree_scroll = ttk.Scrollbar(preview_frame, orient=tk.VERTICAL, command=self.tree.yview)
        tree_scroll.pack(side=tk.RIGHT, fill=tk.Y)
        self.tree.configure(yscrollcommand=tree_scroll.set)
        
        # Configuration section
        config_frame = ttk.LabelFrame(self.data_frame, text="Clustering Configuration", padding=10)
        config_frame.pack(fill=tk.X, padx=10, pady=5)
        
        # Parameters
        params_frame = ttk.Frame(config_frame)
        params_frame.pack(fill=tk.X)
        
        # Left column - Common parameters
        left_col = ttk.Frame(params_frame)
        left_col.pack(side=tk.LEFT, fill=tk.X, expand=True)
        
        ttk.Label(left_col, text="Number of Clusters (K):").grid(row=0, column=0, sticky=tk.W, pady=2)
        self.k_var = tk.IntVar(value=4)
        ttk.Spinbox(left_col, from_=2, to=20, textvariable=self.k_var, width=10).grid(row=0, column=1, sticky=tk.W, pady=2)
        
        ttk.Label(left_col, text="PCA Variance Threshold:").grid(row=1, column=0, sticky=tk.W, pady=2)
        self.pca_variance_var = tk.DoubleVar(value=0.95)
        ttk.Spinbox(left_col, from_=0.8, to=0.99, increment=0.01, textvariable=self.pca_variance_var, width=10).grid(row=1, column=1, sticky=tk.W, pady=2)
        
        ttk.Label(left_col, text="Random Seed:").grid(row=2, column=0, sticky=tk.W, pady=2)
        self.seed_var = tk.IntVar(value=42)
        ttk.Spinbox(left_col, from_=1, to=1000, textvariable=self.seed_var, width=10).grid(row=2, column=1, sticky=tk.W, pady=2)
        
        ttk.Label(left_col, text="K-Means Max Iterations:").grid(row=3, column=0, sticky=tk.W, pady=2)
        self.kmeans_max_iter_var = tk.IntVar(value=300)
        ttk.Spinbox(left_col, from_=100, to=1000, textvariable=self.kmeans_max_iter_var, width=10).grid(row=3, column=1, sticky=tk.W, pady=2)
        
        # Right column - GWO specific parameters
        right_col = ttk.Frame(params_frame)
        right_col.pack(side=tk.RIGHT, fill=tk.X, expand=True)
        
        ttk.Label(right_col, text="GWO Wolves:").grid(row=0, column=0, sticky=tk.W, pady=2)
        self.wolves_var = tk.IntVar(value=25)
        ttk.Spinbox(right_col, from_=10, to=100, textvariable=self.wolves_var, width=10).grid(row=0, column=1, sticky=tk.W, pady=2)
        
        ttk.Label(right_col, text="GWO Iterations:").grid(row=1, column=0, sticky=tk.W, pady=2)
        self.gwo_iterations_var = tk.IntVar(value=25)
        ttk.Spinbox(right_col, from_=10, to=200, textvariable=self.gwo_iterations_var, width=10).grid(row=1, column=1, sticky=tk.W, pady=2)
        
        ttk.Label(right_col, text="Local Refine Steps:").grid(row=2, column=0, sticky=tk.W, pady=2)
        self.refine_var = tk.IntVar(value=1)
        ttk.Spinbox(right_col, from_=0, to=5, textvariable=self.refine_var, width=10).grid(row=2, column=1, sticky=tk.W, pady=2)
        
        ttk.Label(right_col, text="Early Stop Patience:").grid(row=3, column=0, sticky=tk.W, pady=2)
        self.patience_var = tk.IntVar(value=6)
        ttk.Spinbox(right_col, from_=3, to=20, textvariable=self.patience_var, width=10).grid(row=3, column=1, sticky=tk.W, pady=2)
        
        # Run buttons
        run_frame = ttk.Frame(config_frame)
        run_frame.pack(fill=tk.X, pady=10)
        
        self.process_data_button = ttk.Button(run_frame, text="1. Process Data & PCA", 
                                            command=self.process_and_pca)
        self.process_data_button.pack(side=tk.LEFT, padx=5)
        
        self.run_kmeans_button = ttk.Button(run_frame, text="2. Run K-Means", 
                                            command=self.run_standard_kmeans, state='disabled')
        self.run_kmeans_button.pack(side=tk.LEFT, padx=5)
        
        self.run_gwo_button = ttk.Button(run_frame, text="3. Run GWO-K-Means", 
                                        command=self.run_gwo_optimization, state='disabled')
        self.run_gwo_button.pack(side=tk.LEFT, padx=5)
        
        self.run_all_button = ttk.Button(run_frame, text="ðŸš€ Run Complete Analysis", 
                                        command=self.run_complete_analysis, state='disabled')
        self.run_all_button.pack(side=tk.LEFT, padx=10)
        
        self.progress = ttk.Progressbar(run_frame, mode='indeterminate')
        self.progress.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=10)

    def setup_pca_tab(self):
        self.pca_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.pca_frame, text="PCA Analysis")
        
        # PCA Results
        pca_results_frame = ttk.LabelFrame(self.pca_frame, text="PCA Analysis Results", padding=10)
        pca_results_frame.pack(fill=tk.X, padx=10, pady=5)
        
        self.pca_results_text = tk.Text(pca_results_frame, height=12, width=120)
        self.pca_results_text.pack(fill=tk.X)
        
        # PCA Visualization
        pca_viz_frame = ttk.LabelFrame(self.pca_frame, text="PCA Visualizations", padding=10)
        pca_viz_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        
        # Matplotlib canvas for PCA
        self.pca_fig = Figure(figsize=(16, 10))
        self.pca_canvas = FigureCanvasTkAgg(self.pca_fig, pca_viz_frame)
        self.pca_canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        
        # PCA Control buttons
        pca_controls = ttk.Frame(pca_viz_frame)
        pca_controls.pack(fill=tk.X, pady=5)
        
        ttk.Button(pca_controls, text="Show Explained Variance", 
                    command=self.show_explained_variance).pack(side=tk.LEFT, padx=5)
        ttk.Button(pca_controls, text="Show PCA Components", 
                    command=self.show_pca_components).pack(side=tk.LEFT, padx=5)
        ttk.Button(pca_controls, text="Show Cumulative Variance", 
                    command=self.show_cumulative_variance).pack(side=tk.LEFT, padx=5)
        ttk.Button(pca_controls, text="Show 2D Projection", 
                    command=self.show_2d_projection).pack(side=tk.LEFT, padx=5)
        ttk.Button(pca_controls, text="Show PCA Biplot", 
                    command=self.show_pca_biplot).pack(side=tk.LEFT, padx=5)

    def setup_results_tab(self):
        self.results_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.results_frame, text="Clustering Results")
        
        # Results comparison
        comparison_frame = ttk.LabelFrame(self.results_frame, text="Performance Comparison Summary", padding=10)
        comparison_frame.pack(fill=tk.X, padx=10, pady=5)
        
        self.comparison_text = tk.Text(comparison_frame, height=15, width=120)
        self.comparison_text.pack(fill=tk.X)
        
        # Visualization area
        viz_frame = ttk.LabelFrame(self.results_frame, text="Clustering Visualizations", padding=10)
        viz_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        
        # Matplotlib canvas
        self.fig = Figure(figsize=(18, 12))
        self.canvas = FigureCanvasTkAgg(self.fig, viz_frame)
        self.canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        
        # Control buttons for different visualizations
        viz_controls = ttk.Frame(viz_frame)
        viz_controls.pack(fill=tk.X, pady=5)
        
        ttk.Button(viz_controls, text="Side-by-Side Clustering", 
                    command=self.show_side_by_side_clusters).pack(side=tk.LEFT, padx=5)
        ttk.Button(viz_controls, text="Metrics Comparison", 
                    command=self.show_metrics_comparison).pack(side=tk.LEFT, padx=5)
        ttk.Button(viz_controls, text="Convergence Analysis", 
                    command=self.show_convergence_analysis).pack(side=tk.LEFT, padx=5)
        ttk.Button(viz_controls, text="Quality Heatmap", 
                    command=self.show_quality_heatmap).pack(side=tk.LEFT, padx=5)
        ttk.Button(viz_controls, text="Cluster Quality Analysis", 
                    command=self.show_cluster_quality_analysis).pack(side=tk.LEFT, padx=5)

    def setup_evaluation_tab(self):
        self.evaluation_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.evaluation_frame, text="Detailed Evaluation")
        
        # Detailed evaluation results
        eval_frame = ttk.LabelFrame(self.evaluation_frame, text="Comprehensive Metrics Evaluation", padding=10)
        eval_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        
        self.evaluation_text = scrolledtext.ScrolledText(eval_frame, height=35, width=140)
        self.evaluation_text.pack(fill=tk.BOTH, expand=True)

    def setup_best_position_tab(self):
        self.best_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.best_frame, text="Best Method Determination")
        
        # Best method results
        best_results_frame = ttk.LabelFrame(self.best_frame, text="Best Method Analysis & Recommendation", padding=10)
        best_results_frame.pack(fill=tk.X, padx=10, pady=5)
        
        self.best_results_text = tk.Text(best_results_frame, height=18, width=140)
        self.best_results_text.pack(fill=tk.X)
        
        # Final visualization
        final_viz_frame = ttk.LabelFrame(self.best_frame, text="Final Optimal Clustering Visualization", padding=10)
        final_viz_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        
        # Matplotlib canvas for best result
        self.best_fig = Figure(figsize=(16, 10))
        self.best_canvas = FigureCanvasTkAgg(self.best_fig, final_viz_frame)
        self.best_canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        
        # Best result controls
        best_controls = ttk.Frame(final_viz_frame)
        best_controls.pack(fill=tk.X, pady=5)
        
        ttk.Button(best_controls, text="Show Best Result", 
                    command=self.show_best_result).pack(side=tk.LEFT, padx=5)
        ttk.Button(best_controls, text="Final Dashboard", 
                    command=self.show_final_dashboard).pack(side=tk.LEFT, padx=5)
        ttk.Button(best_controls, text="Export Results", 
                    command=self.export_best_results).pack(side=tk.LEFT, padx=5)

    def setup_logs_tab(self):
        self.logs_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.logs_frame, text="Process Logs")
        
        # Log text area
        log_frame = ttk.LabelFrame(self.logs_frame, text="Detailed Process Logs", padding=10)
        log_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        
        self.log_text = scrolledtext.ScrolledText(log_frame, height=35, width=120)
        self.log_text.pack(fill=tk.BOTH, expand=True)
        
        # Clear logs button
        ttk.Button(log_frame, text="Clear Logs", 
                    command=lambda: self.log_text.delete(1.0, tk.END)).pack(pady=5)

    # Data handling methods
    def upload_file(self):
        file_path = filedialog.askopenfilename(
            title="Select Data File",
            filetypes=[("Excel files", "*.xlsx *.xls"), ("CSV files", "*.csv"), ("All files", "*.*")]
        )
        
        if file_path:
            try:
                # Load data
                if file_path.endswith(('.xlsx', '.xls')):
                    self.df = pd.read_excel(file_path)
                else:
                    self.df = pd.read_csv(file_path)
                
                self.file_label.config(text=f"Loaded: {os.path.basename(file_path)}")
                self.log(f"Data loaded successfully: {self.df.shape[0]} rows, {self.df.shape[1]} columns")
                
                # Update preview
                self.update_data_preview()
                
                # Enable process button
                self.process_data_button.config(state='normal')
                
            except Exception as e:
                messagebox.showerror("Error", f"Failed to load file:\n{str(e)}")
                self.log(f"Error loading file: {str(e)}")

    def update_data_preview(self):
        # Clear existing data
        for item in self.tree.get_children():
            self.tree.delete(item)
        
        if self.df is not None:
            # Configure columns
            cols = list(self.df.columns)
            self.tree["columns"] = cols
            self.tree["show"] = "headings"
            
            # Configure column headings and widths
            for col in cols:
                self.tree.heading(col, text=col)
                self.tree.column(col, width=100)
            
            # Add data (first 100 rows)
            for idx, row in self.df.head(100).iterrows():
                values = [str(row[col]) for col in cols]
                self.tree.insert("", "end", values=values)

    # Core processing methods
    def process_and_pca(self):
        if self.df is None:
            messagebox.showerror("Error", "Please upload data first!")
            return
        
        self._disable_buttons()
        
        threading.Thread(target=self._process_and_pca, daemon=True).start()

    def _process_and_pca(self):
        try:
            self.log("="*80)
            self.log("STARTING DATA PREPROCESSING AND PCA ANALYSIS")
            self.log("="*80)
            
            # Data preprocessing
            self.log("Step 1: Data Preprocessing...")
            self._preprocess_data()
            
            # PCA Analysis
            self.log("Step 2: Principal Component Analysis...")
            self._perform_pca()
            
            # Update PCA results display
            self.root.after(0, self._update_pca_display)
            
            # Switch to PCA tab
            self.root.after(0, lambda: self.notebook.select(1))
            
            # Enable clustering buttons
            self.root.after(0, lambda: (
                self.run_kmeans_button.config(state='normal'),
                self.run_gwo_button.config(state='normal'),
                self.run_all_button.config(state='normal')
            ))
            
            self.log("Data preprocessing and PCA analysis completed successfully!")
            
        except Exception as e:
            self.log(f"Error in preprocessing/PCA: {str(e)}")
            self.root.after(0, lambda: messagebox.showerror("Processing Error", str(e)))
        
        finally:
            self.root.after(0, self._enable_buttons)

    def _preprocess_data(self):
        # Remove gender column if exists
        cols_drop = [c for c in ["gender"] if c in self.df.columns]
        if cols_drop:
            self.df = self.df.drop(columns=cols_drop)
            self.log(f"Dropped columns: {cols_drop}")
        
        # Handle binary columns (Ya/Tidak -> 1/0)
        binary_cols = []
        other_object_cols = []
        
        map_yes_no = {
            "ya": 1, "y": 1, "yes": 1, "true": 1, True: 1, 1: 1,
            "tidak": 0, "tdk": 0, "no": 0, "n": 0, "false": 0, False: 0, 0: 0
        }
        
        for col in self.df.columns:
            if self.df[col].dtype == object:
                vals = self.df[col].dropna().astype(str).str.lower().str.strip().unique()
                uniq = set(vals)
                yes_set = {"ya", "y", "yes", "true"}
                no_set = {"tidak", "tdk", "no", "n", "false"}
                
                if all((v in yes_set or v in no_set) for v in uniq) and len(uniq) <= 2:
                    binary_cols.append(col)
                else:
                    other_object_cols.append(col)
        
        # Encode binary columns
        for col in binary_cols:
            self.df[col] = self.df[col].astype(str).str.lower().str.strip().map(map_yes_no)
        
        # Drop non-numeric, non-binary columns
        if other_object_cols:
            self.df = self.df.drop(columns=other_object_cols)
            self.log(f"Dropped non-numeric columns: {other_object_cols}")
        
        # Select numeric columns
        num_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()
        self.log(f"Final numeric columns: {len(num_cols)} features")
        
        # Store original preprocessed data
        self.X_original = self.df[num_cols].copy()
        
        # Imputation and scaling
        imputer = SimpleImputer(strategy="median")
        X_num = imputer.fit_transform(self.df[num_cols])
        
        scaler = MinMaxScaler()
        self.X_preprocessed = scaler.fit_transform(X_num)
        
        self.log(f"Data preprocessing completed: {self.X_preprocessed.shape}")

    def _perform_pca(self):
        variance_threshold = self.pca_variance_var.get()
        
        # PCA with specified variance threshold
        self.pca_model = PCA(n_components=variance_threshold, svd_solver="full", random_state=42)
        self.X_pca = self.pca_model.fit_transform(self.X_preprocessed)
        
        # PCA for 2D visualization
        self.pca2_model = PCA(n_components=2, random_state=42)
        self.X_pca2 = self.pca2_model.fit_transform(self.X_preprocessed)
        
        # Store explained variance ratios
        self.explained_variance_ratio = self.pca_model.explained_variance_ratio_
        
        self.log(f"PCA Analysis Results:")
        self.log(f"  - Variance threshold: {variance_threshold:.1%}")
        self.log(f"  - Components selected: {self.X_pca.shape[1]}")
        self.log(f"  - Total variance explained: {self.explained_variance_ratio.sum():.4f}")
        self.log(f"  - Dimensionality reduction: {self.X_preprocessed.shape[1]} â†’ {self.X_pca.shape[1]}")

    # Complete GWO Algorithm Implementation
    def assign_labels(self, X, C):
        return np.argmin(pairwise_distances(X, C, metric="euclidean"), axis=1)

    def wcss(self, X, C):
        lbl = self.assign_labels(X, C)
        d2 = pairwise_distances(X, C, metric="euclidean")**2
        return float(np.sum(d2[np.arange(X.shape[0]), lbl]))

    def repair_empty_clusters(self, X, C):
        k = C.shape[0]
        labels = self.assign_labels(X, C)
        for j in range(k):
            if np.sum(labels == j) == 0:
                D = pairwise_distances(X, C, metric="euclidean")
                idx = np.argmax(D.min(axis=1))
                C[j] = X[idx]
        return C

    def mini_lloyd_local(self, X, C0, steps=1):
        C = C0.copy()
        for _ in range(steps):
            C = self.repair_empty_clusters(X, C)
            lbl = self.assign_labels(X, C)
            for j in range(C.shape[0]):
                pts = X[lbl == j]
                if len(pts) > 0:
                    C[j] = pts.mean(axis=0)
        return self.wcss(X, C), C

    def gwo_only_with_history(self, X, k, wolves=24, iters=25, random_state=42, 
                                local_refine_steps=1, patience=6, reinit_rate=0.15):
        rng = np.random.RandomState(random_state)
        n, d = X.shape
        xmin, xmax = X.min(axis=0), X.max(axis=0)
        eps = 1e-12
        
        def rand_centroids():
            return xmin + rng.rand(k, d) * (xmax - xmin + eps)
        
        # Initialize population
        P = np.stack([rand_centroids() for _ in range(wolves)], axis=0)
        for i in range(wolves):
            _, P[i] = self.mini_lloyd_local(X, P[i], steps=1)
        
        fitness = np.array([self.wcss(X, P[i]) for i in range(wolves)])
        order = np.argsort(fitness)
        alpha = P[order[0]].copy()
        f_alpha = fitness[order[0]]
        beta = P[order[1]].copy() if wolves > 1 else alpha.copy()
        delta = P[order[2]].copy() if wolves > 2 else alpha.copy()
        
        history = []
        iter_times = []
        metrics_hist = []
        best_so_far = f_alpha
        no_improve = 0
        
        t0_global = time.perf_counter()
        
        for t in range(1, iters + 1):
            t_iter0 = time.perf_counter()
            a = 2 - 2 * (t - 1) / max(1, iters - 1)
            
            for i in range(wolves):
                r1, r2 = rng.rand(k, d), rng.rand(k, d)
                A1 = 2 * a * r1 - a
                C1 = 2 * r2
                D_alpha = np.abs(C1 * alpha - P[i])
                X1 = alpha - A1 * D_alpha
                
                r1, r2 = rng.rand(k, d), rng.rand(k, d)
                A2 = 2 * a * r1 - a
                C2 = 2 * r2
                D_beta = np.abs(C2 * beta - P[i])
                X2 = beta - A2 * D_beta
                
                r1, r2 = rng.rand(k, d), rng.rand(k, d)
                A3 = 2 * a * r1 - a
                C3 = 2 * r2
                D_delta = np.abs(C3 * delta - P[i])
                X3 = delta - A3 * D_delta
                
                P[i] = (X1 + X2 + X3) / 3.0
                P[i] = np.minimum(np.maximum(P[i], xmin), xmax)
                P[i] = self.repair_empty_clusters(X, P[i])
                
                if local_refine_steps > 0:
                    _, P[i] = self.mini_lloyd_local(X, P[i], steps=local_refine_steps)
            
            fitness = np.array([self.wcss(X, P[i]) for i in range(wolves)])
            order = np.argsort(fitness)
            alpha_new = P[order[0]].copy()
            f_alpha_new = fitness[order[0]]
            beta = P[order[1]].copy() if wolves > 1 else alpha_new.copy()
            delta = P[order[2]].copy() if wolves > 2 else alpha_new.copy()
            
            # Reinitialize worst wolves
            n_re = max(1, int(reinit_rate * wolves))
            for wi in order[-n_re:]:
                P[wi] = rand_centroids()
                _, P[wi] = self.mini_lloyd_local(X, P[wi], steps=1)
            
            alpha, f_alpha = alpha_new, f_alpha_new
            history.append((t, alpha.copy()))
            
            labels_alpha = self.assign_labels(X, alpha)
            try:
                sil = silhouette_score(X, labels_alpha)
                ch = calinski_harabasz_score(X, labels_alpha)
                db = davies_bouldin_score(X, labels_alpha)
            except:
                sil = ch = db = np.nan
            
            metrics_hist.append((sil, ch, db, f_alpha))
            
            t_iter = time.perf_counter() - t_iter0
            iter_times.append(t_iter)
            
            self.log(f"[GWO] iter={t:02d} | time={t_iter:.3f}s | best_WCSS={f_alpha:.6f}")
            
            # Early stopping
            if f_alpha + 1e-9 < best_so_far:
                best_so_far = f_alpha
                no_improve = 0
            else:
                no_improve += 1
                if no_improve >= patience:
                    self.log(f"[GWO] Early stop (no improvement {patience} iters).")
                    break
        
        total_time = time.perf_counter() - t0_global
        return alpha, history, iter_times, total_time, metrics_hist

    def best_from_history(self, X, history):
        best = None
        for it, C in history:
            val = self.wcss(X, C)
            if best is None or val < best[2]:
                best = (it, C.copy(), val)
        return best

    # Clustering execution methods
    def run_complete_analysis(self):
        if self.X_pca is None:
            messagebox.showerror("Error", "Please process data and run PCA first!")
            return
        
        self._disable_buttons()
        
        threading.Thread(target=self._run_complete_analysis, daemon=True).start()

    def _run_complete_analysis(self):
        self.log("\n" + "="*80)
        self.log("STARTING COMPLETE CLUSTERING ANALYSIS")
        self.log("="*80)
        
        # Run Standard K-Means first
        self.log("\nPhase 1: Standard K-Means Analysis")
        self.log("-"*50)
        self._run_standard_kmeans()
        
        self.log("\nPhase 2: GWO-K-Means Analysis")
        self.log("-"*50)
        # Run GWO-K-Means
        self._run_gwo_optimization()
        
        self.log("\nPhase 3: Comparative Analysis")
        self.log("-"*50)
        # Determine best method
        self._determine_best_method()
        
        self.log("="*80)
        self.log("COMPLETE ANALYSIS FINISHED")
        self.log("="*80)
        
        # Switch to results tab
        self.root.after(0, lambda: self.notebook.select(4))  # Best position tab

    def run_standard_kmeans(self):
        if self.X_pca is None:
            messagebox.showerror("Error", "Please process data and run PCA first!")
            return
        
        self._disable_buttons()
        
        threading.Thread(target=self._run_standard_kmeans, daemon=True).start()

    def _run_standard_kmeans(self):
        try:
            self.log("Starting Standard K-Means clustering...")
            
            k = self.k_var.get()
            seed = self.seed_var.get()
            max_iter = self.kmeans_max_iter_var.get()
            
            self.log(f"Standard K-Means Parameters:")
            self.log(f"  - K (clusters): {k}")
            self.log(f"  - Max iterations: {max_iter}")
            self.log(f"  - Random seed: {seed}")
            
            start_time = time.perf_counter()
            
            # Run K-Means with multiple initializations
            best_kmeans = None
            best_inertia = float('inf')
            iterations_history = []
            
            self.log("Running multiple K-Means initializations...")
            for init_run in range(10):
                kmeans = KMeans(n_clusters=k, random_state=seed+init_run, 
                                max_iter=max_iter, n_init=1, algorithm='lloyd')
                kmeans.fit(self.X_pca)
                
                if kmeans.inertia_ < best_inertia:
                    best_inertia = kmeans.inertia_
                    best_kmeans = kmeans
                    
                iterations_history.append((init_run+1, kmeans.n_iter_, kmeans.inertia_))
                self.log(f"  Init {init_run+1}: {kmeans.n_iter_} iterations, WCSS: {kmeans.inertia_:.6f}")
            
            total_time = time.perf_counter() - start_time
            
            self.kmeans_labels = best_kmeans.labels_
            self.kmeans_centroids = best_kmeans.cluster_centers_
            
            # Calculate metrics
            sil = silhouette_score(self.X_pca, self.kmeans_labels)
            ch = calinski_harabasz_score(self.X_pca, self.kmeans_labels)
            db = davies_bouldin_score(self.X_pca, self.kmeans_labels)
            
            # Store results
            self.kmeans_results = {
                'inertia': best_kmeans.inertia_,
                'silhouette': sil,
                'calinski_harabasz': ch,
                'davies_bouldin': db,
                'total_time': total_time,
                'n_iter': best_kmeans.n_iter_,
                'iterations_history': iterations_history,
                'method': 'Standard K-Means',
                'converged': best_kmeans.n_iter_ < max_iter
            }
            
            self.log(f"Standard K-Means Results:")
            self.log(f"  - Runtime: {total_time:.3f} seconds")
            self.log(f"  - WCSS: {best_kmeans.inertia_:.6f}")
            self.log(f"  - Silhouette: {sil:.6f}")
            
            # Update displays
            self.root.after(0, self._update_results_displays)
            
        except Exception as e:
            self.log(f"Error in K-Means: {str(e)}")
            self.root.after(0, lambda: messagebox.showerror("K-Means Error", str(e)))
        
        finally:
            self.root.after(0, self._enable_buttons)

    def run_gwo_optimization(self):
        if self.X_pca is None:
            messagebox.showerror("Error", "Please process data and run PCA first!")
            return
        
        self._disable_buttons()
        
        threading.Thread(target=self._run_gwo_optimization, daemon=True).start()

    def _run_gwo_optimization(self):
        try:
            self.log("Starting GWO-K-Means optimization...")
            
            # Get parameters
            k = self.k_var.get()
            wolves = self.wolves_var.get()
            iters = self.gwo_iterations_var.get()
            local_refine = self.refine_var.get()
            patience = self.patience_var.get()
            seed = self.seed_var.get()
            
            self.log(f"GWO-K-Means Parameters:")
            self.log(f"  - K (clusters): {k}")
            self.log(f"  - Wolves (population): {wolves}")
            self.log(f"  - Max iterations: {iters}")
            
            # Run GWO optimization
            best_centroids, history, iter_times, total_time, metrics_hist = self.gwo_only_with_history(
                self.X_pca, k, wolves, iters, seed, local_refine, patience
            )
            
            # Get best solution from history
            best_it, C_best, best_wcss = self.best_from_history(self.X_pca, history)
            self.gwo_labels = self.assign_labels(self.X_pca, C_best)
            self.gwo_centroids = C_best
            
            # Calculate final metrics
            sil = silhouette_score(self.X_pca, self.gwo_labels)
            ch = calinski_harabasz_score(self.X_pca, self.gwo_labels)
            db = davies_bouldin_score(self.X_pca, self.gwo_labels)
            
            # Store results
            self.gwo_results = {
                'best_iteration': best_it,
                'best_wcss': best_wcss,
                'silhouette': sil,
                'calinski_harabasz': ch,
                'davies_bouldin': db,
                'total_time': total_time,
                'history': history,
                'metrics_history': metrics_hist,
                'iter_times': iter_times,
                'method': 'GWO-K-Means',
                'total_iterations': len(history)
            }
            
            self.log(f"GWO-K-Means Results:")
            self.log(f"  - Runtime: {total_time:.3f} seconds")
            self.log(f"  - WCSS: {best_wcss:.6f}")
            self.log(f"  - Silhouette: {sil:.6f}")
            
            # Update results display
            self.root.after(0, self._update_results_displays)
            
            self.log("GWO-K-Means optimization completed successfully!")
            
        except Exception as e:
            self.log(f"Error in GWO optimization: {str(e)}")
            self.root.after(0, lambda: messagebox.showerror("GWO Optimization Error", str(e)))
        
        finally:
            self.root.after(0, self._enable_buttons)

    # Method comparison and result determination
    def _determine_best_method(self):
        if not (self.kmeans_results and self.gwo_results):
            return
        
        self.log("Determining best clustering method...")
        
        # Score calculation based on normalized metrics
        kmeans_scores = []
        gwo_scores = []
        
        # Silhouette (higher better)
        sil_max = max(self.kmeans_results['silhouette'], self.gwo_results['silhouette'])
        sil_min = min(self.kmeans_results['silhouette'], self.gwo_results['silhouette'])
        sil_range = sil_max - sil_min if sil_max != sil_min else 1
        
        kmeans_sil_score = (self.kmeans_results['silhouette'] - sil_min) / sil_range * 0.30
        gwo_sil_score = (self.gwo_results['silhouette'] - sil_min) / sil_range * 0.30
        
        # Calinski-Harabasz (higher better)
        ch_max = max(self.kmeans_results['calinski_harabasz'], self.gwo_results['calinski_harabasz'])
        ch_min = min(self.kmeans_results['calinski_harabasz'], self.gwo_results['calinski_harabasz'])
        ch_range = ch_max - ch_min if ch_max != ch_min else 1
        
        kmeans_ch_score = (self.kmeans_results['calinski_harabasz'] - ch_min) / ch_range * 0.30
        gwo_ch_score = (self.gwo_results['calinski_harabasz'] - ch_min) / ch_range * 0.30
        
        # Davies-Bouldin (lower better)
        db_max = max(self.kmeans_results['davies_bouldin'], self.gwo_results['davies_bouldin'])
        db_min = min(self.kmeans_results['davies_bouldin'], self.gwo_results['davies_bouldin'])
        db_range = db_max - db_min if db_max != db_min else 1
        
        kmeans_db_score = (db_max - self.kmeans_results['davies_bouldin']) / db_range * 0.25
        gwo_db_score = (db_max - self.gwo_results['davies_bouldin']) / db_range * 0.25
        
        # WCSS (lower better)
        wcss_max = max(self.kmeans_results['inertia'], self.gwo_results['best_wcss'])
        wcss_min = min(self.kmeans_results['inertia'], self.gwo_results['best_wcss'])
        wcss_range = wcss_max - wcss_min if wcss_max != wcss_min else 1
        
        kmeans_wcss_score = (wcss_max - self.kmeans_results['inertia']) / wcss_range * 0.15
        gwo_wcss_score = (wcss_max - self.gwo_results['best_wcss']) / wcss_range * 0.15
        
        # Total scores
        kmeans_total = kmeans_sil_score + kmeans_ch_score + kmeans_db_score + kmeans_wcss_score
        gwo_total = gwo_sil_score + gwo_ch_score + gwo_db_score + gwo_wcss_score
        
        self.log(f"Scoring Results:")
        self.log(f"  K-Means Total Score: {kmeans_total:.4f}")
        self.log(f"  GWO-K-Means Total Score: {gwo_total:.4f}")
        
        # Determine winner
        if gwo_total > kmeans_total:
            self.best_method = 'GWO-K-Means'
            self.best_score = gwo_total
            score_diff = gwo_total - kmeans_total
        else:
            self.best_method = 'Standard K-Means'
            self.best_score = kmeans_total
            score_diff = kmeans_total - gwo_total
        
        self.log(f"WINNER: {self.best_method} (Score difference: {score_diff:.4f})")
        
        # Update all displays
        self.root.after(0, self._update_all_displays)

    # Visualization methods
    def show_explained_variance(self):
        """Show PCA explained variance analysis"""
        if self.explained_variance_ratio is None:
            return
        
        self.pca_fig.clear()
        
        # Create 2x2 subplot layout
        components_to_show = min(20, len(self.explained_variance_ratio))
        
        # Individual component variance
        ax1 = self.pca_fig.add_subplot(221)
        bars = ax1.bar(range(1, components_to_show + 1), 
                        self.explained_variance_ratio[:components_to_show], 
                        color='steelblue', alpha=0.7, edgecolor='black')
        
        ax1.set_xlabel('Principal Component')
        ax1.set_ylabel('Explained Variance Ratio')
        ax1.set_title('Individual Component Variance')
        ax1.grid(axis='y', alpha=0.3)
        
        # Cumulative variance
        ax2 = self.pca_fig.add_subplot(222)
        cumsum = np.cumsum(self.explained_variance_ratio)
        ax2.plot(range(1, len(cumsum) + 1), cumsum, 'bo-', linewidth=2, markersize=4)
        ax2.axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='95% threshold')
        ax2.axhline(y=self.pca_variance_var.get(), color='green', linestyle='--', alpha=0.7, label=f'{self.pca_variance_var.get():.0%} selected')
        ax2.set_xlabel('Number of Components')
        ax2.set_ylabel('Cumulative Explained Variance')
        ax2.set_title('Cumulative Variance Explanation')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # Scree plot
        ax3 = self.pca_fig.add_subplot(223)
        ax3.plot(range(1, components_to_show + 1), self.explained_variance_ratio[:components_to_show], 'ro-')
        ax3.set_xlabel('Principal Component')
        ax3.set_ylabel('Explained Variance Ratio')
        ax3.set_title('Scree Plot')
        ax3.grid(True, alpha=0.3)
        
        # Kaiser criterion
        ax4 = self.pca_fig.add_subplot(224)
        eigenvalues = self.explained_variance_ratio * len(self.explained_variance_ratio)
        colors = ['red' if ev > 1.0 else 'blue' for ev in eigenvalues[:components_to_show]]
        bars = ax4.bar(range(1, components_to_show + 1), eigenvalues[:components_to_show], color=colors, alpha=0.7)
        ax4.axhline(y=1.0, color='black', linestyle='--', alpha=0.7, label='Kaiser criterion')
        ax4.set_xlabel('Principal Component')
        ax4.set_ylabel('Eigenvalue (Approximate)')
        ax4.set_title('Kaiser Criterion Analysis')
        ax4.legend()
        ax4.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        self.pca_canvas.draw()

    def show_pca_components(self):
        """Show PCA components heatmap"""
        if self.pca_model is None:
            return
        
        self.pca_fig.clear()
        
        # Components heatmap
        ax1 = self.pca_fig.add_subplot(121)
        components_to_show = min(10, self.pca_model.components_.shape[0])
        features_to_show = min(20, self.pca_model.components_.shape[1])
        
        im = ax1.imshow(self.pca_model.components_[:components_to_show, :features_to_show], 
                        aspect='auto', cmap='RdBu_r')
        ax1.set_xlabel('Original Features')
        ax1.set_ylabel('Principal Components')
        ax1.set_title('PCA Components Loading Matrix')
        self.pca_fig.colorbar(im, ax=ax1)
        
        # Feature contributions to first two PCs
        ax2 = self.pca_fig.add_subplot(122)
        if self.pca_model.components_.shape[0] >= 2:
            pc1_loadings = self.pca_model.components_[0, :]
            pc2_loadings = self.pca_model.components_[1, :]
            
            scatter = ax2.scatter(pc1_loadings, pc2_loadings, alpha=0.6)
            ax2.set_xlabel(f'PC1 Loadings (Var: {self.explained_variance_ratio[0]:.3f})')
            ax2.set_ylabel(f'PC2 Loadings (Var: {self.explained_variance_ratio[1]:.3f})')
            ax2.set_title('Feature Loadings on PC1 vs PC2')
            ax2.grid(True, alpha=0.3)
            ax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)
            ax2.axvline(x=0, color='k', linestyle='-', alpha=0.3)
        
        plt.tight_layout()
        self.pca_canvas.draw()

    def show_cumulative_variance(self):
        """Show cumulative variance plot"""
        if self.explained_variance_ratio is None:
            return
        
        self.pca_fig.clear()
        ax = self.pca_fig.add_subplot(111)
        
        cumsum = np.cumsum(self.explained_variance_ratio)
        ax.plot(range(1, len(cumsum) + 1), cumsum, 'bo-', linewidth=3, markersize=6)
        
        # Add threshold lines
        ax.axhline(y=0.80, color='orange', linestyle='--', alpha=0.7, label='80% threshold')
        ax.axhline(y=0.90, color='yellow', linestyle='--', alpha=0.7, label='90% threshold')
        ax.axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='95% threshold')
        ax.axhline(y=self.pca_variance_var.get(), color='green', linestyle='-', linewidth=3, 
                    alpha=0.8, label=f'{self.pca_variance_var.get():.0%} selected')
        
        # Mark selected point
        selected_components = len(self.explained_variance_ratio)
        ax.scatter([selected_components], [cumsum[selected_components-1]], 
                    color='green', s=200, marker='*', zorder=5, 
                    label=f'{selected_components} components')
        
        ax.set_xlabel('Number of Principal Components')
        ax.set_ylabel('Cumulative Explained Variance Ratio')
        ax.set_title('PCA Cumulative Explained Variance')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_ylim(0, 1.05)
        
        plt.tight_layout()
        self.pca_canvas.draw()

    def show_2d_projection(self):
        """Show 2D PCA projection"""
        if self.X_pca2 is None:
            return
        
        self.pca_fig.clear()
        ax = self.pca_fig.add_subplot(111)
        
        scatter = ax.scatter(self.X_pca2[:, 0], self.X_pca2[:, 1], alpha=0.6, s=30)
        ax.set_xlabel(f'PC1 ({self.pca2_model.explained_variance_ratio_[0]:.3f} variance)')
        ax.set_ylabel(f'PC2 ({self.pca2_model.explained_variance_ratio_[1]:.3f} variance)')
        ax.set_title(f'PCA 2D Projection\n({self.pca2_model.explained_variance_ratio_.sum():.3f} total variance)')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        self.pca_canvas.draw()

    def show_pca_biplot(self):
        """Show PCA biplot with loadings"""
        if self.pca2_model is None:
            return
        
        self.pca_fig.clear()
        ax = self.pca_fig.add_subplot(111)
        
        # Plot data points
        scatter = ax.scatter(self.X_pca2[:, 0], self.X_pca2[:, 1], alpha=0.6, s=30)
        
        # Plot loadings as arrows if we have feature names
        if hasattr(self, 'X_original') and self.X_original is not None:
            loadings = self.pca2_model.components_.T * np.sqrt(self.pca2_model.explained_variance_)
            
            n_features_show = min(10, loadings.shape[0])
            feature_names = self.X_original.columns[:n_features_show] if hasattr(self.X_original, 'columns') else [f'Feature_{i}' for i in range(n_features_show)]
            
            loading_scale = max(np.abs(self.X_pca2).max(axis=0)) / max(np.abs(loadings[:n_features_show]).max(axis=0))
            
            for i in range(n_features_show):
                ax.arrow(0, 0, loadings[i, 0] * loading_scale, loadings[i, 1] * loading_scale,
                        head_width=0.05, head_length=0.05, fc='red', ec='red', alpha=0.8)
                ax.text(loadings[i, 0] * loading_scale * 1.1, loadings[i, 1] * loading_scale * 1.1,
                        feature_names[i], fontsize=9, ha='center', va='center',
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))
        
        ax.set_xlabel(f'PC1 ({self.pca2_model.explained_variance_ratio_[0]:.3f} variance)')
        ax.set_ylabel(f'PC2 ({self.pca2_model.explained_variance_ratio_[1]:.3f} variance)')
        ax.set_title('PCA Biplot with Feature Loadings')
        ax.grid(True, alpha=0.3)
        ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)
        ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)
        
        plt.tight_layout()
        self.pca_canvas.draw()

    def show_side_by_side_clusters(self):
        """Show side-by-side clustering comparison"""
        if self.kmeans_labels is None and self.gwo_labels is None:
            return
        
        self.fig.clear()
        
        if self.kmeans_labels is not None and self.gwo_labels is not None:
            # Side by side comparison
            ax1 = self.fig.add_subplot(121)
            scatter1 = ax1.scatter(self.X_pca2[:, 0], self.X_pca2[:, 1], c=self.kmeans_labels, 
                                    cmap='tab10', alpha=0.7, s=50)
            
            # K-means centroids in 2D projection
            k_centers2d = np.array([self.X_pca2[self.kmeans_labels == c].mean(axis=0) 
                                    for c in np.unique(self.kmeans_labels)])
            ax1.scatter(k_centers2d[:, 0], k_centers2d[:, 1], s=300, marker='X', 
                        c='red', edgecolor='black', linewidth=2, label='Centroids')
            
            ax1.set_title(f'Standard K-Means\nWCSS: {self.kmeans_results["inertia"]:.4f}\nSilhouette: {self.kmeans_results["silhouette"]:.4f}')
            ax1.set_xlabel(f'PC1 ({self.pca2_model.explained_variance_ratio_[0]:.3f})')
            ax1.set_ylabel(f'PC2 ({self.pca2_model.explained_variance_ratio_[1]:.3f})')
            ax1.grid(True, alpha=0.3)
            ax1.legend()
            
            ax2 = self.fig.add_subplot(122)
            scatter2 = ax2.scatter(self.X_pca2[:, 0], self.X_pca2[:, 1], c=self.gwo_labels, 
                                    cmap='tab10', alpha=0.7, s=50)
            
            # GWO centroids in 2D projection  
            g_centers2d = np.array([self.X_pca2[self.gwo_labels == c].mean(axis=0) 
                                    for c in np.unique(self.gwo_labels)])
            ax2.scatter(g_centers2d[:, 0], g_centers2d[:, 1], s=300, marker='X', 
                        c='red', edgecolor='black', linewidth=2, label='Centroids')
            
            ax2.set_title(f'GWO-K-Means\nWCSS: {self.gwo_results["best_wcss"]:.4f}\nSilhouette: {self.gwo_results["silhouette"]:.4f}')
            ax2.set_xlabel(f'PC1 ({self.pca2_model.explained_variance_ratio_[0]:.3f})')
            ax2.set_ylabel(f'PC2 ({self.pca2_model.explained_variance_ratio_[1]:.3f})')
            ax2.grid(True, alpha=0.3)
            ax2.legend()
            
        elif self.kmeans_labels is not None:
            ax = self.fig.add_subplot(111)
            scatter = ax.scatter(self.X_pca2[:, 0], self.X_pca2[:, 1], c=self.kmeans_labels, 
                                cmap='tab10', alpha=0.7, s=50)
            centers2d = np.array([self.X_pca2[self.kmeans_labels == c].mean(axis=0) 
                                for c in np.unique(self.kmeans_labels)])
            ax.scatter(centers2d[:, 0], centers2d[:, 1], s=300, marker='X', 
                        c='red', edgecolor='black', linewidth=2)
            ax.set_title('Standard K-Means Clustering')
            ax.grid(True, alpha=0.3)
            
        elif self.gwo_labels is not None:
            ax = self.fig.add_subplot(111)
            scatter = ax.scatter(self.X_pca2[:, 0], self.X_pca2[:, 1], c=self.gwo_labels, 
                                cmap='tab10', alpha=0.7, s=50)
            centers2d = np.array([self.X_pca2[self.gwo_labels == c].mean(axis=0) 
                                for c in np.unique(self.gwo_labels)])
            ax.scatter(centers2d[:, 0], centers2d[:, 1], s=300, marker='X', 
                        c='red', edgecolor='black', linewidth=2)
            ax.set_title('GWO-K-Means Clustering')
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        self.canvas.draw()

    def show_metrics_comparison(self):
        """Show metrics comparison visualization"""
        if not (self.kmeans_results and self.gwo_results):
            return
        
        self.fig.clear()
        
        # Create 2x2 subplot for comprehensive comparison
        ax1 = self.fig.add_subplot(221)
        
        metrics = ['Silhouette', 'Calinski-H\n(scaled)', 'Davies-B\n(inverted)']
        kmeans_values = [
            self.kmeans_results['silhouette'],
            self.kmeans_results['calinski_harabasz'] / 1000,
            1 / self.kmeans_results['davies_bouldin']
        ]
        gwo_values = [
            self.gwo_results['silhouette'],
            self.gwo_results['calinski_harabasz'] / 1000,
            1 / self.gwo_results['davies_bouldin']
        ]
        
        x = np.arange(len(metrics))
        width = 0.35
        
        bars1 = ax1.bar(x - width/2, kmeans_values, width, label='K-Means', 
                        alpha=0.8, color='skyblue')
        bars2 = ax1.bar(x + width/2, gwo_values, width, label='GWO-K-Means', 
                        alpha=0.8, color='orange')
        
        ax1.set_xlabel('Quality Metrics')
        ax1.set_ylabel('Metric Values')
        ax1.set_title('Quality Metrics Comparison')
        ax1.set_xticks(x)
        ax1.set_xticklabels(metrics)
        ax1.legend()
        ax1.grid(axis='y', alpha=0.3)
        
        # WCSS comparison
        ax2 = self.fig.add_subplot(222)
        methods = ['K-Means', 'GWO-K-Means']
        wcss_values = [self.kmeans_results['inertia'], self.gwo_results['best_wcss']]
        colors = ['skyblue', 'orange']
        bars = ax2.bar(methods, wcss_values, color=colors, alpha=0.8)
        
        for bar, val in zip(bars, wcss_values):
            ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(wcss_values)*0.01,
                    f'{val:.4f}', ha='center', va='bottom', fontweight='bold')
        
        ax2.set_ylabel('WCSS (Lower is Better)')
        ax2.set_title('Within-Cluster Sum of Squares')
        ax2.grid(axis='y', alpha=0.3)
        
        # Runtime comparison
        ax3 = self.fig.add_subplot(223)
        times = [self.kmeans_results['total_time'], self.gwo_results['total_time']]
        bars = ax3.bar(methods, times, color=colors, alpha=0.8)
        
        for bar, time_val in zip(bars, times):
            ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(times)*0.01,
                    f'{time_val:.3f}s', ha='center', va='bottom', fontweight='bold')
        
        ax3.set_ylabel('Execution Time (seconds)')
        ax3.set_title('Computational Efficiency')
        ax3.grid(axis='y', alpha=0.3)
        
        # Performance improvement percentages
        ax4 = self.fig.add_subplot(224)
        
        # Calculate improvements
        wcss_improvement = (self.kmeans_results['inertia'] - self.gwo_results['best_wcss']) / self.kmeans_results['inertia'] * 100
        sil_improvement = (self.gwo_results['silhouette'] - self.kmeans_results['silhouette']) / abs(self.kmeans_results['silhouette']) * 100
        ch_improvement = (self.gwo_results['calinski_harabasz'] - self.kmeans_results['calinski_harabasz']) / self.kmeans_results['calinski_harabasz'] * 100
        db_improvement = (self.kmeans_results['davies_bouldin'] - self.gwo_results['davies_bouldin']) / self.kmeans_results['davies_bouldin'] * 100
        
        improvements = [wcss_improvement, sil_improvement, ch_improvement, db_improvement]
        improvement_labels = ['WCSS\nReduction', 'Silhouette\nIncrease', 'Calinski-H\nIncrease', 'Davies-B\nReduction']
        colors_imp = ['green' if imp > 0 else 'red' for imp in improvements]
        
        bars = ax4.bar(improvement_labels, improvements, color=colors_imp, alpha=0.7)
        ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        ax4.set_ylabel('Improvement (%)')
        ax4.set_title('GWO vs K-Means Improvements')
        ax4.grid(axis='y', alpha=0.3)
        
        for bar, imp in zip(bars, improvements):
            ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + (max(improvements) - min(improvements))*0.02 * (1 if imp > 0 else -1),
                    f'{imp:+.1f}%', ha='center', va='bottom' if imp > 0 else 'top', fontweight='bold')
        
        plt.tight_layout()
        self.canvas.draw()

    def show_convergence_analysis(self):
        """Show convergence analysis for both methods"""
        if not (self.kmeans_results and self.gwo_results):
            return
        
        self.fig.clear()
        
        # Create subplot layout
        gs = self.fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)
        
        # GWO convergence plot
        if 'metrics_history' in self.gwo_results:
            ax1 = self.fig.add_subplot(gs[0, :])
            
            # Extract metrics from history
            iterations = list(range(1, len(self.gwo_results['metrics_history']) + 1))
            wcss_values = [wcss for _, _, _, wcss in self.gwo_results['metrics_history']]
            sil_values = [sil for sil, _, _, _ in self.gwo_results['metrics_history']]
            
            # Plot WCSS convergence
            ax1_twin = ax1.twinx()
            
            line1 = ax1.plot(iterations, wcss_values, 'b-', linewidth=2, label='WCSS', marker='o', markersize=3)
            line2 = ax1_twin.plot(iterations, sil_values, 'r-', linewidth=2, label='Silhouette', marker='s', markersize=3)
            
            # Mark best iteration
            best_iter = self.gwo_results['best_iteration']
            ax1.axvline(x=best_iter, color='green', linestyle='--', alpha=0.7, label=f'Best at iter {best_iter}')
            
            ax1.set_xlabel('Iteration')
            ax1.set_ylabel('WCSS', color='b')
            ax1_twin.set_ylabel('Silhouette Score', color='r')
            ax1.set_title('GWO-K-Means Convergence Analysis')
            ax1.grid(True, alpha=0.3)
            
            # Combine legends
            lines1, labels1 = ax1.get_legend_handles_labels()
            lines2, labels2 = ax1_twin.get_legend_handles_labels()
            ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right')
        
        # Runtime comparison
        ax2 = self.fig.add_subplot(gs[1, 0])
        methods = ['K-Means', 'GWO-K-Means']
        times = [self.kmeans_results['total_time'], self.gwo_results['total_time']]
        colors = ['skyblue', 'orange']
        
        bars = ax2.bar(methods, times, color=colors, alpha=0.7, edgecolor='black')
        
        for bar, time_val in zip(bars, times):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(times)*0.01,
                    f'{time_val:.3f}s', ha='center', va='bottom', fontweight='bold')
        
        ax2.set_ylabel('Execution Time (seconds)')
        ax2.set_title('Computational Efficiency')
        ax2.grid(axis='y', alpha=0.3)
        
        # Performance summary
        ax3 = self.fig.add_subplot(gs[1, 1])
        ax3.axis('off')
        
        summary_text = "PERFORMANCE SUMMARY\n" + "-"*30 + "\n"
        if hasattr(self, 'best_method') and self.best_method:
            summary_text += f"Winner: {self.best_method}\n"
            summary_text += f"Score: {self.best_score:.4f}\n\n"
        
        summary_text += f"K-Means:\n"
        summary_text += f"  WCSS: {self.kmeans_results['inertia']:.4f}\n"
        summary_text += f"  Silhouette: {self.kmeans_results['silhouette']:.4f}\n"
        summary_text += f"  Time: {self.kmeans_results['total_time']:.3f}s\n\n"
        
        summary_text += f"GWO-K-Means:\n"
        summary_text += f"  WCSS: {self.gwo_results['best_wcss']:.4f}\n"
        summary_text += f"  Silhouette: {self.gwo_results['silhouette']:.4f}\n"
        summary_text += f"  Time: {self.gwo_results['total_time']:.3f}s\n"
        
        ax3.text(0.1, 0.9, summary_text, transform=ax3.transAxes, fontsize=10,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        plt.tight_layout()
        self.canvas.draw()

    def show_quality_heatmap(self):
        """Show quality metrics heatmap comparison"""
        if not (self.kmeans_results and self.gwo_results):
            return
        
        self.fig.clear()
        
        # Create metrics comparison heatmap
        metrics_data = {
            'Silhouette': [self.kmeans_results['silhouette'], self.gwo_results['silhouette']],
            'Calinski-H': [self.kmeans_results['calinski_harabasz']/1000, 
                            self.gwo_results['calinski_harabasz']/1000],
            'Davies-B\n(inverted)': [1/self.kmeans_results['davies_bouldin'], 
                                    1/self.gwo_results['davies_bouldin']],
            'WCSS\n(inverted)': [1/self.kmeans_results['inertia']*10000, 
                                1/self.gwo_results['best_wcss']*10000]
        }
        
        # Create DataFrame for heatmap
        df_metrics = pd.DataFrame(metrics_data, index=['K-Means', 'GWO-K-Means'])
        
        # Create heatmap
        ax = self.fig.add_subplot(111)
        
        # Normalize data for better color representation
        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()
        df_normalized = pd.DataFrame(
            scaler.fit_transform(df_metrics.T).T,
            index=df_metrics.index,
            columns=df_metrics.columns
        )
        
        # Create heatmap
        im = ax.imshow(df_normalized.values, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)
        
        # Set ticks and labels
        ax.set_xticks(range(len(df_metrics.columns)))
        ax.set_yticks(range(len(df_metrics.index)))
        ax.set_xticklabels(df_metrics.columns, rotation=45, ha='right')
        ax.set_yticklabels(df_metrics.index)
        
        # Add text annotations with actual values
        for i in range(len(df_metrics.index)):
            for j in range(len(df_metrics.columns)):
                original_val = df_metrics.iloc[i, j]
                normalized_val = df_normalized.iloc[i, j]
                
                # Choose text color based on background
                text_color = 'white' if normalized_val < 0.5 else 'black'
                
                ax.text(j, i, f'{original_val:.3f}', ha='center', va='center',
                        color=text_color, fontweight='bold')
        
        # Add colorbar
        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
        cbar.set_label('Normalized Score (0=Worst, 1=Best)', rotation=270, labelpad=20)
        
        ax.set_title('Clustering Quality Metrics Heatmap')
        
        plt.tight_layout()
        self.canvas.draw()

    def show_cluster_quality_analysis(self):
        """Show detailed cluster quality analysis"""
        if not (self.kmeans_labels is not None or self.gwo_labels is not None):
            return
        
        self.fig.clear()
        
        methods_data = []
        if self.kmeans_labels is not None:
            methods_data.append(('K-Means', self.kmeans_labels, 'skyblue'))
        if self.gwo_labels is not None:
            methods_data.append(('GWO-K-Means', self.gwo_labels, 'orange'))
        
        n_methods = len(methods_data)
        
        if n_methods == 2:
            # Create 2x2 subplot for detailed analysis
            fig = self.fig
            
            # Cluster size comparison
            ax1 = fig.add_subplot(221)
            
            for i, (method, labels, color) in enumerate(methods_data):
                unique_labels, counts = np.unique(labels, return_counts=True)
                x_pos = np.arange(len(unique_labels)) + i * 0.35
                
                bars = ax1.bar(x_pos, counts, 0.35, label=method, color=color, alpha=0.7)
                
                # Add count labels
                for bar, count in zip(bars, counts):
                    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,
                            str(count), ha='center', va='bottom', fontsize=9)
            
            ax1.set_xlabel('Cluster ID')
            ax1.set_ylabel('Number of Samples')
            ax1.set_title('Cluster Size Distribution Comparison')
            ax1.legend()
            ax1.grid(axis='y', alpha=0.3)
            
            # Cluster balance analysis
            ax2 = fig.add_subplot(222)
            
            balance_scores = []
            method_names = []
            
            for method, labels, color in methods_data:
                unique_labels, counts = np.unique(labels, return_counts=True)
                balance_score = 1 - (np.std(counts) / np.mean(counts))
                balance_scores.append(balance_score)
                method_names.append(method)
            
            bars = ax2.bar(method_names, balance_scores, color=[data[2] for data in methods_data], alpha=0.7)
            
            for bar, score in zip(bars, balance_scores):
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
            
            ax2.set_ylabel('Balance Score (1.0 = Perfect)')
            ax2.set_title('Cluster Balance Comparison')
            ax2.set_ylim(0, 1.1)
            ax2.grid(axis='y', alpha=0.3)
            
            # Silhouette analysis per cluster
            ax3 = fig.add_subplot(223)
            
            y_pos = 0
            colors = ['skyblue', 'orange']
            
            for idx, (method, labels, color) in enumerate(methods_data):
                silhouette_vals = silhouette_samples(self.X_pca, labels)
                unique_labels = np.unique(labels)
                
                for cluster_id in unique_labels:
                    cluster_silhouette_vals = silhouette_vals[labels == cluster_id]
                    cluster_silhouette_vals.sort()
                    
                    size_cluster = len(cluster_silhouette_vals)
                    y_upper = y_pos + size_cluster
                    
                    ax3.fill_betweenx(np.arange(y_pos, y_upper), 0, cluster_silhouette_vals,
                                        facecolor=color, alpha=0.7)
                    
                    # Add cluster label
                    ax3.text(-0.05, y_pos + 0.5 * size_cluster, f'{method[0]}{cluster_id}',
                            fontsize=8, ha='right')
                    
                    y_pos = y_upper + 10
            
            # Add average silhouette scores
            if self.kmeans_results:
                ax3.axvline(x=self.kmeans_results['silhouette'], color='red', linestyle='--', 
                            label=f'K-Means avg: {self.kmeans_results["silhouette"]:.3f}')
            if self.gwo_results:
                ax3.axvline(x=self.gwo_results['silhouette'], color='green', linestyle='--',
                            label=f'GWO avg: {self.gwo_results["silhouette"]:.3f}')
            
            ax3.set_xlabel('Silhouette Score')
            ax3.set_ylabel('Cluster Samples')
            ax3.set_title('Silhouette Analysis by Cluster')
            ax3.legend()
            ax3.grid(axis='x', alpha=0.3)
            
            # Quality metrics radar
            ax4 = fig.add_subplot(224, projection='polar')
            
            categories = ['Silhouette', 'Calinski-H', 'Davies-B\n(inv)', 'WCSS\n(inv)']
            
            # Normalize values between 0 and 1
            kmeans_norm = [
                (self.kmeans_results['silhouette'] + 1) / 2,
                min(1.0, self.kmeans_results['calinski_harabasz'] / 1000),
                1 / (1 + self.kmeans_results['davies_bouldin']),
                1 / (1 + self.kmeans_results['inertia'] / 1000)
            ]
            
            gwo_norm = [
                (self.gwo_results['silhouette'] + 1) / 2,
                min(1.0, self.gwo_results['calinski_harabasz'] / 1000),
                1 / (1 + self.gwo_results['davies_bouldin']),
                1 / (1 + self.gwo_results['best_wcss'] / 1000)
            ]
            
            # Close the radar chart
            kmeans_norm += [kmeans_norm[0]]
            gwo_norm += [gwo_norm[0]]
            
            angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
            angles += [angles[0]]
            
            ax4.plot(angles, kmeans_norm, 'o-', linewidth=2, label='K-Means', color='skyblue')
            ax4.fill(angles, kmeans_norm, alpha=0.25, color='skyblue')
            ax4.plot(angles, gwo_norm, 'o-', linewidth=2, label='GWO-K-Means', color='orange')
            ax4.fill(angles, gwo_norm, alpha=0.25, color='orange')
            
            ax4.set_xticks(angles[:-1])
            ax4.set_xticklabels(categories)
            ax4.set_ylim(0, 1)
            ax4.set_title('Quality Radar Chart')
            ax4.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
            ax4.grid(True)
        
        else:
            # Single method analysis
            method, labels, color = methods_data[0]
            
            ax1 = self.fig.add_subplot(221)
            unique_labels, counts = np.unique(labels, return_counts=True)
            bars = ax1.bar(unique_labels, counts, color=color, alpha=0.7)
            
            for bar, count in zip(bars, counts):
                percentage = count / len(labels) * 100
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,
                        f'{count}\n({percentage:.1f}%)', ha='center', va='bottom')
            
            ax1.set_xlabel('Cluster ID')
            ax1.set_ylabel('Number of Samples')
            ax1.set_title(f'{method} Cluster Distribution')
            ax1.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        self.canvas.draw()

    # Best result visualization methods
    def show_best_result(self):
        if not hasattr(self, 'best_method') or not self.best_method:
            return
        
        self.best_fig.clear()
        
        if self.best_method == 'GWO-K-Means':
            best_labels = self.gwo_labels
            best_results = self.gwo_results
            color_scheme = 'orange'
        else:
            best_labels = self.kmeans_labels
            best_results = self.kmeans_results
            color_scheme = 'skyblue'
        
        # Main clustering visualization
        ax1 = self.best_fig.add_subplot(221)
        scatter = ax1.scatter(self.X_pca2[:, 0], self.X_pca2[:, 1], c=best_labels, 
                            cmap='tab10', alpha=0.7, s=60)
        
        # Plot centroids
        centers2d = np.array([self.X_pca2[best_labels == c].mean(axis=0) 
                            for c in np.unique(best_labels)])
        ax1.scatter(centers2d[:, 0], centers2d[:, 1], s=400, marker='*', 
                    c='red', edgecolor='black', linewidth=2, label='Centroids')
        
        for i, (cx, cy) in enumerate(centers2d):
            ax1.text(cx, cy, f'C{i}', ha='center', va='center', fontweight='bold', 
                    fontsize=12, color='white')
        
        ax1.set_xlabel(f'PC1 ({self.pca2_model.explained_variance_ratio_[0]:.3f} var)')
        ax1.set_ylabel(f'PC2 ({self.pca2_model.explained_variance_ratio_[1]:.3f} var)')
        ax1.set_title(f'OPTIMAL CLUSTERING RESULT\n{self.best_method}')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # Cluster size distribution
        ax2 = self.best_fig.add_subplot(222)
        unique_labels, counts = np.unique(best_labels, return_counts=True)
        bars = ax2.bar(unique_labels, counts, color=color_scheme, alpha=0.8, edgecolor='black')
        
        for bar, count in zip(bars, counts):
            percentage = count / len(best_labels) * 100
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01, 
                    f'{count}\n({percentage:.1f}%)', ha='center', va='bottom', fontweight='bold')
        
        ax2.set_xlabel('Cluster ID')
        ax2.set_ylabel('Number of Samples')
        ax2.set_title('Optimal Cluster Distribution')
        ax2.set_xticks(unique_labels)
        ax2.set_xticklabels([f'C{i}' for i in unique_labels])
        ax2.grid(axis='y', alpha=0.3)
        
        # Quality metrics display
        ax3 = self.best_fig.add_subplot(223)
        
        if 'best_wcss' in best_results:
            wcss_val = best_results['best_wcss']
        else:
            wcss_val = best_results['inertia']
        
        metrics = ['Silhouette', 'Calinski-H', 'Davies-B', 'WCSS']
        values = [best_results['silhouette'], best_results['calinski_harabasz']/1000,
                    best_results['davies_bouldin'], wcss_val/1000]
        colors = ['green' if v > 0.5 else 'orange' if v > 0.25 else 'red' for v in 
                    [best_results['silhouette'], 0.8, 1/best_results['davies_bouldin'], 0.8]]
        
        bars = ax3.bar(metrics, values, color=colors, alpha=0.8, edgecolor='black')
        
        for bar, val in zip(bars, values):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.02,
                    f'{val:.4f}', ha='center', va='bottom', fontweight='bold')
        
        ax3.set_ylabel('Metric Values')
        ax3.set_title('Optimal Quality Metrics')
        ax3.grid(axis='y', alpha=0.3)
        
        # Quality assessment
        ax4 = self.best_fig.add_subplot(224)
        
        sil_val = best_results['silhouette']
        if sil_val > 0.7:
            quality_score = 'Excellent'
            quality_color = 'green'
        elif sil_val > 0.5:
            quality_score = 'Good'
            quality_color = 'lightgreen'
        elif sil_val > 0.25:
            quality_score = 'Fair'
            quality_color = 'yellow'
        else:
            quality_score = 'Poor'
            quality_color = 'red'
        
        ax4.text(0.5, 0.7, 'CLUSTERING QUALITY', ha='center', va='center', 
                fontsize=16, fontweight='bold', transform=ax4.transAxes)
        ax4.text(0.5, 0.5, quality_score, ha='center', va='center', 
                fontsize=24, fontweight='bold', color=quality_color, transform=ax4.transAxes)
        ax4.text(0.5, 0.3, f'Silhouette: {sil_val:.4f}', ha='center', va='center', 
                fontsize=14, transform=ax4.transAxes)
        ax4.text(0.5, 0.2, f'Score: {self.best_score:.4f}/1.0000', ha='center', va='center', 
                fontsize=12, fontweight='bold', transform=ax4.transAxes)
        
        ax4.set_xlim(0, 1)
        ax4.set_ylim(0, 1)
        ax4.axis('off')
        
        # Add colored background
        ax4.add_patch(plt.Rectangle((0.1, 0.1), 0.8, 0.8, 
                                    facecolor=quality_color, alpha=0.2, transform=ax4.transAxes))
        
        plt.tight_layout()
        self.best_canvas.draw()

    def show_final_dashboard(self):
        if not hasattr(self, 'best_method') or not self.best_method:
            return
        
        self.best_fig.clear()
        
        # Create a comprehensive dashboard
        gs = self.best_fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
        
        # Title
        self.best_fig.suptitle('FINAL CLUSTERING ANALYSIS DASHBOARD', fontsize=16, fontweight='bold')
        
        # Best clustering result (large plot)
        ax_main = self.best_fig.add_subplot(gs[0:2, 0:2])
        
        if self.best_method == 'GWO-K-Means':
            best_labels = self.gwo_labels
            best_results = self.gwo_results
        else:
            best_labels = self.kmeans_labels
            best_results = self.kmeans_results
        
        scatter = ax_main.scatter(self.X_pca2[:, 0], self.X_pca2[:, 1], c=best_labels, 
                                cmap='tab10', alpha=0.7, s=50)
        
        centers2d = np.array([self.X_pca2[best_labels == c].mean(axis=0) 
                            for c in np.unique(best_labels)])
        ax_main.scatter(centers2d[:, 0], centers2d[:, 1], s=300, marker='*', 
                        c='red', edgecolor='black', linewidth=2)
        
        ax_main.set_xlabel(f'PC1 ({self.pca2_model.explained_variance_ratio_[0]:.3f})')
        ax_main.set_ylabel(f'PC2 ({self.pca2_model.explained_variance_ratio_[1]:.3f})')
        ax_main.set_title(f'OPTIMAL CLUSTERING: {self.best_method}')
        ax_main.grid(True, alpha=0.3)
        
        # Additional dashboard components would go here
        plt.tight_layout()
        self.best_canvas.draw()

    def export_best_results(self):
        if not hasattr(self, 'best_method') or not self.best_method:
            messagebox.showwarning("Warning", "No results to export!")
            return
        
        file_path = filedialog.asksaveasfilename(
            title="Export Best Results",
            defaultextension=".csv",
            filetypes=[("CSV files", "*.csv"), ("Excel files", "*.xlsx")]
        )
        
        if file_path:
            try:
                if self.best_method == 'GWO-K-Means':
                    best_labels = self.gwo_labels
                    best_results = self.gwo_results
                else:
                    best_labels = self.kmeans_labels
                    best_results = self.kmeans_results
                
                # Create results DataFrame
                results_df = pd.DataFrame({
                    'Sample_ID': range(len(best_labels)),
                    'Cluster_Label': best_labels
                })
                
                # Add PCA coordinates
                for i in range(min(5, self.X_pca.shape[1])):
                    results_df[f'PCA_Component_{i+1}'] = self.X_pca[:, i]
                
                results_df['PCA_2D_X'] = self.X_pca2[:, 0]
                results_df['PCA_2D_Y'] = self.X_pca2[:, 1]
                
                # Save file
                if file_path.endswith('.xlsx'):
                    results_df.to_excel(file_path, index=False)
                else:
                    results_df.to_csv(file_path, index=False)
                
                messagebox.showinfo("Success", f"Results exported successfully to:\n{file_path}")
                self.log(f"Best results exported to: {file_path}")
                
            except Exception as e:
                messagebox.showerror("Export Error", f"Failed to export results:\n{str(e)}")
                self.log(f"Export error: {str(e)}")

    # Display update methods
    def _update_pca_display(self):
        pca_text = "PRINCIPAL COMPONENT ANALYSIS (PCA) RESULTS\n" + "="*100 + "\n\n"
        
        pca_text += f"PCA CONFIGURATION:\n" + "-"*60 + "\n"
        pca_text += f"Original Features: {self.X_preprocessed.shape[1]}\n"
        pca_text += f"Variance Threshold: {self.pca_variance_var.get():.1%}\n"
        pca_text += f"Selected Components: {self.X_pca.shape[1]}\n"
        pca_text += f"Data Samples: {self.X_pca.shape[0]}\n"
        pca_text += f"Total Variance Explained: {self.explained_variance_ratio.sum():.6f}\n\n"
        
        pca_text += f"VARIANCE EXPLANATION:\n" + "-"*60 + "\n"
        for i, var_exp in enumerate(self.explained_variance_ratio[:10]):
            pca_text += f"PC{i+1:2d}: {var_exp:.6f} ({var_exp:.2%})\n"
        
        if len(self.explained_variance_ratio) > 10:
            pca_text += f"... and {len(self.explained_variance_ratio)-10} more components\n"
        
        self.pca_results_text.delete(1.0, tk.END)
        self.pca_results_text.insert(1.0, pca_text)
        
        # Show initial PCA visualization
        self.show_explained_variance()

    def _update_results_displays(self):
        self._update_comparison_display()
        self._update_evaluation_display()
        if hasattr(self, 'best_method') and self.best_method:
            self._update_best_position_display()

    def _update_all_displays(self):
        self._update_comparison_display()
        self._update_evaluation_display()
        self._update_best_position_display()

    def _update_comparison_display(self):
        comparison_text = "COMPREHENSIVE CLUSTERING COMPARISON ANALYSIS\n" + "="*100 + "\n\n"
        
        if self.kmeans_results:
            comparison_text += f"STANDARD K-MEANS RESULTS:\n" + "-"*70 + "\n"
            comparison_text += f"Runtime: {self.kmeans_results['total_time']:.4f} seconds\n"
            comparison_text += f"WCSS: {self.kmeans_results['inertia']:.8f}\n"
            comparison_text += f"Silhouette: {self.kmeans_results['silhouette']:.8f}\n"
            comparison_text += f"Calinski-Harabasz: {self.kmeans_results['calinski_harabasz']:.8f}\n"
            comparison_text += f"Davies-Bouldin: {self.kmeans_results['davies_bouldin']:.8f}\n\n"
        
        if self.gwo_results:
            comparison_text += f"GWO-K-MEANS RESULTS:\n" + "-"*70 + "\n"
            comparison_text += f"Runtime: {self.gwo_results['total_time']:.4f} seconds\n"
            comparison_text += f"WCSS: {self.gwo_results['best_wcss']:.8f}\n"
            comparison_text += f"Silhouette: {self.gwo_results['silhouette']:.8f}\n"
            comparison_text += f"Calinski-Harabasz: {self.gwo_results['calinski_harabasz']:.8f}\n"
            comparison_text += f"Davies-Bouldin: {self.gwo_results['davies_bouldin']:.8f}\n\n"
        
        if hasattr(self, 'best_method') and self.best_method:
            comparison_text += f"WINNER: {self.best_method}\n"
            comparison_text += f"Composite Score: {self.best_score:.4f}\n"
        
        self.comparison_text.delete(1.0, tk.END)
        self.comparison_text.insert(1.0, comparison_text)

    def _update_evaluation_display(self):
        eval_text = "COMPREHENSIVE CLUSTERING QUALITY EVALUATION\n" + "="*120 + "\n\n"
        
        eval_text += "CLUSTERING QUALITY METRICS EXPLANATION:\n" + "-"*90 + "\n"
        eval_text += "â€¢ Silhouette Coefficient: Measures cluster separation quality\n"
        eval_text += "â€¢ Calinski-Harabasz Index: Ratio of between to within cluster dispersion\n"
        eval_text += "â€¢ Davies-Bouldin Index: Average similarity between clusters\n"
        eval_text += "â€¢ WCSS: Within-cluster sum of squares (compactness measure)\n\n"
        
        if self.kmeans_results and self.gwo_results:
            eval_text += "DETAILED PERFORMANCE COMPARISON:\n" + "-"*90 + "\n"
            
            # Calculate improvements
            wcss_improvement = (self.kmeans_results['inertia'] - self.gwo_results['best_wcss']) / self.kmeans_results['inertia'] * 100
            sil_improvement = (self.gwo_results['silhouette'] - self.kmeans_results['silhouette']) / abs(self.kmeans_results['silhouette']) * 100
            
            eval_text += f"WCSS Improvement: {wcss_improvement:+.3f}%\n"
            eval_text += f"Silhouette Improvement: {sil_improvement:+.3f}%\n"
        
        self.evaluation_text.delete(1.0, tk.END)
        self.evaluation_text.insert(1.0, eval_text)

    def _update_best_position_display(self):
        if not hasattr(self, 'best_method') or not self.best_method:
            return
        
        best_text = "OPTIMAL CLUSTERING METHOD DETERMINATION\n" + "="*120 + "\n\n"
        
        best_text += f"WINNER: {self.best_method.upper()}\n" + "-"*80 + "\n"
        best_text += f"Overall Quality Score: {self.best_score:.6f} / 1.0000\n\n"
        
        if self.best_method == 'GWO-K-Means':
            best_results = self.gwo_results
            best_labels = self.gwo_labels
        else:
            best_results = self.kmeans_results
            best_labels = self.kmeans_labels
        
        best_text += f"OPTIMAL CLUSTERING CONFIGURATION:\n" + "-"*80 + "\n"
        best_text += f"Method: {self.best_method}\n"
        best_text += f"Number of Clusters: {self.k_var.get()}\n"
        best_text += f"Total Data Points: {len(best_labels)}\n\n"
        
        best_text += f"OPTIMAL PERFORMANCE METRICS:\n" + "-"*80 + "\n"
        if 'best_wcss' in best_results:
            best_text += f"WCSS: {best_results['best_wcss']:.10f}\n"
        else:
            best_text += f"WCSS: {best_results['inertia']:.10f}\n"
        
        best_text += f"Silhouette: {best_results['silhouette']:.10f}\n"
        best_text += f"Calinski-Harabasz: {best_results['calinski_harabasz']:.10f}\n"
        best_text += f"Davies-Bouldin: {best_results['davies_bouldin']:.10f}\n"
        best_text += f"Runtime: {best_results['total_time']:.6f} seconds\n"
        
        self.best_results_text.delete(1.0, tk.END)
        self.best_results_text.insert(1.0, best_text)
        
        # Show the best clustering result
        self.show_best_result()

    # Utility methods
    def _disable_buttons(self):
        self.process_data_button.config(state='disabled')
        self.run_kmeans_button.config(state='disabled')
        self.run_gwo_button.config(state='disabled')
        self.run_all_button.config(state='disabled')
        self.progress.start()

    def _enable_buttons(self):
        self.process_data_button.config(state='normal')
        if self.X_pca is not None:
            self.run_kmeans_button.config(state='normal')
            self.run_gwo_button.config(state='normal')
            self.run_all_button.config(state='normal')
        self.progress.stop()

    def log(self, message):
        timestamp = time.strftime("%H:%M:%S")
        log_message = f"[{timestamp}] {message}\n"
        self.log_text.insert(tk.END, log_message)
        self.log_text.see(tk.END)
        self.root.update()


    def main():
    root = tk.Tk()
    app = GWOKMeansApp(root)
    root.mainloop()


    if __name__ == "__main__":
    main()